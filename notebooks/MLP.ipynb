{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f9c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josephlcy/workplace/MSBD5008/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gzip\n",
    "import pickle\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0010b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickle file\n",
    "with gzip.open('data/combined_graph_with3embedding_processed.pkl.gz', 'rb') as f:\n",
    "    joinned_graph = pickle.load(f)\n",
    "\n",
    "with gzip.open('data/combined_graph_with3embedding_processed_0.pkl.gz', 'rb') as f:\n",
    "    mag_graph = pickle.load(f)\n",
    "\n",
    "with gzip.open('data/combined_graph_with3embedding_processed_1.pkl.gz', 'rb') as f:\n",
    "    arxiv_graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90f5419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([369343, 389]),\n",
       " torch.Size([200000, 389]),\n",
       " torch.Size([169343, 389]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinned_graph.ndata['label'].shape, mag_graph.ndata['label'].shape, arxiv_graph.ndata['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfac1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(graph, feat):\n",
    "    \"\"\"\n",
    "    Get the indices of the training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train_idx = graph.ndata['train_mask'].nonzero(as_tuple=True)[0]\n",
    "    val_idx = graph.ndata['valid_mask'].nonzero(as_tuple=True)[0]\n",
    "    test_idx = graph.ndata['test_mask'].nonzero(as_tuple=True)[0]\n",
    "\n",
    "    feat_map = {\n",
    "        'orig': 'feat',\n",
    "        'e5': 'e5_feat',\n",
    "        'ga': 'ga_embedding'\n",
    "    }\n",
    "\n",
    "    # Original features\n",
    "    X_train = graph.ndata[feat_map[feat]][train_idx]\n",
    "    X_test = graph.ndata[feat_map[feat]][test_idx]\n",
    "    X_val = graph.ndata[feat_map[feat]][val_idx]\n",
    "\n",
    "    # Labels\n",
    "    y_train = graph.ndata['label'][train_idx].argmax(dim=1).long()\n",
    "    y_test = graph.ndata['label'][test_idx].argmax(dim=1).long()\n",
    "    y_val = graph.ndata['label'][val_idx].argmax(dim=1).long()\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    return train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89385dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, dropout_rate=0.3):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_feats, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()  \n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=100):\n",
    "    model.to(device)\n",
    "    best_val_accuracy = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    best_model_state = None\n",
    "    best_val_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            features, labels = batch\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_accuracy, val_f1 = evaluate(model, val_loader, criterion, device)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "              \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_epoch = epoch\n",
    "            best_model_state = model.state_dict()\n",
    "            #print(f'Epoch [{epoch+1}/{num_epochs}]: Best validation accuracy improved to {best_val_accuracy:.4f}. Saving model.')\n",
    "        \n",
    "    return best_model_state, best_val_accuracy, best_val_f1, best_val_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e29119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "in_feats = 128\n",
    "hidden_size = 512\n",
    "num_classes = 389\n",
    "learning_rate = 0.005\n",
    "dropout_rate = 0.3\n",
    "batch_size = 1024\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee8c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(graph, feat, in_feats, hidden_size, num_classes, dropout_rate, batch_size, num_epochs):\n",
    "    \"\"\"\n",
    "    Run the experiment with the specified graph and feature type.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model initialization\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MLP(in_feats, hidden_size, num_classes, dropout_rate).to(device)\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, test_dataset, val_dataset = train_test_val_split(graph, feat)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training\n",
    "    best_model_state, best_val_accuracy, best_val_f1, best_val_epoch = train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs)\n",
    "    \n",
    "    # Load best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "    train_loss, train_accuracy, train_f1 = evaluate(model, train_loader, criterion, device)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy, test_f1 = evaluate(model, test_loader, criterion, device)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test F1: {test_f1:.4f}')\n",
    "\n",
    "    result = {\n",
    "        'Best Val Epoch': best_val_epoch,\n",
    "        'Training Loss (Best Val Model)': train_loss,\n",
    "        'Best_Val_Accuracy': best_val_accuracy,\n",
    "        'Train_Accuracy': train_accuracy,\n",
    "        'Test_Accuracy': test_accuracy,\n",
    "        'Test_f1': test_f1\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48b4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for graph: joinned with feature: orig\n",
      "Epoch [1/100], Train Loss: 3.0223, Validation Loss: 2.3373\n",
      "Epoch [11/100], Train Loss: 2.2915, Validation Loss: 2.1744\n",
      "Epoch [21/100], Train Loss: 2.2205, Validation Loss: 2.1686\n",
      "Epoch [31/100], Train Loss: 2.1814, Validation Loss: 2.1768\n",
      "Epoch [41/100], Train Loss: 2.1558, Validation Loss: 2.1958\n",
      "Epoch [51/100], Train Loss: 2.1348, Validation Loss: 2.2090\n",
      "Epoch [61/100], Train Loss: 2.1213, Validation Loss: 2.2242\n",
      "Epoch [71/100], Train Loss: 2.1122, Validation Loss: 2.2255\n",
      "Epoch [81/100], Train Loss: 2.1080, Validation Loss: 2.2288\n",
      "Epoch [91/100], Train Loss: 2.0967, Validation Loss: 2.2528\n",
      "Test Loss: 2.0686, Test Accuracy: 0.4501, Test F1: 0.0775\n",
      "Running experiment for graph: joinned with feature: e5\n",
      "Epoch [1/100], Train Loss: 2.9115, Validation Loss: 2.3713\n",
      "Epoch [11/100], Train Loss: 2.4801, Validation Loss: 2.3668\n",
      "Epoch [21/100], Train Loss: 2.4441, Validation Loss: 2.3959\n",
      "Epoch [31/100], Train Loss: 2.4258, Validation Loss: 2.4005\n",
      "Epoch [41/100], Train Loss: 2.4073, Validation Loss: 2.4299\n",
      "Epoch [51/100], Train Loss: 2.3945, Validation Loss: 2.4579\n",
      "Epoch [61/100], Train Loss: 2.4033, Validation Loss: 2.4499\n",
      "Epoch [71/100], Train Loss: 2.3921, Validation Loss: 2.4665\n",
      "Epoch [81/100], Train Loss: 2.3913, Validation Loss: 2.4689\n",
      "Epoch [91/100], Train Loss: 2.3892, Validation Loss: 2.4840\n",
      "Test Loss: 3.0041, Test Accuracy: 0.1935, Test F1: 0.0472\n",
      "Running experiment for graph: joinned with feature: ga\n",
      "Epoch [1/100], Train Loss: 2.9832, Validation Loss: 2.7315\n",
      "Epoch [11/100], Train Loss: 2.5236, Validation Loss: 2.7112\n",
      "Epoch [21/100], Train Loss: 2.4796, Validation Loss: 2.7162\n",
      "Epoch [31/100], Train Loss: 2.4508, Validation Loss: 2.7257\n",
      "Epoch [41/100], Train Loss: 2.4496, Validation Loss: 2.7225\n",
      "Epoch [51/100], Train Loss: 2.4261, Validation Loss: 2.7377\n",
      "Epoch [61/100], Train Loss: 2.4180, Validation Loss: 2.7408\n",
      "Epoch [71/100], Train Loss: 2.4138, Validation Loss: 2.7590\n",
      "Epoch [81/100], Train Loss: 2.4131, Validation Loss: 2.7818\n",
      "Epoch [91/100], Train Loss: 2.4123, Validation Loss: 2.8041\n",
      "Test Loss: 3.1042, Test Accuracy: 0.1565, Test F1: 0.0310\n",
      "Running experiment for graph: mag with feature: orig\n",
      "Epoch [1/100], Train Loss: 3.5263, Validation Loss: 2.7800\n",
      "Epoch [11/100], Train Loss: 2.6396, Validation Loss: 2.5354\n",
      "Epoch [21/100], Train Loss: 2.5306, Validation Loss: 2.5380\n",
      "Epoch [31/100], Train Loss: 2.4664, Validation Loss: 2.5385\n",
      "Epoch [41/100], Train Loss: 2.4123, Validation Loss: 2.5740\n",
      "Epoch [51/100], Train Loss: 2.3830, Validation Loss: 2.5989\n",
      "Epoch [61/100], Train Loss: 2.3478, Validation Loss: 2.6209\n",
      "Epoch [71/100], Train Loss: 2.3265, Validation Loss: 2.6365\n",
      "Epoch [81/100], Train Loss: 2.3113, Validation Loss: 2.6424\n",
      "Epoch [91/100], Train Loss: 2.2981, Validation Loss: 2.6657\n",
      "Test Loss: 2.6720, Test Accuracy: 0.3372, Test F1: 0.0502\n",
      "Running experiment for graph: mag with feature: e5\n",
      "Epoch [1/100], Train Loss: 3.2229, Validation Loss: 2.6447\n",
      "Epoch [11/100], Train Loss: 2.5913, Validation Loss: 2.6217\n",
      "Epoch [21/100], Train Loss: 2.4958, Validation Loss: 2.6485\n",
      "Epoch [31/100], Train Loss: 2.4413, Validation Loss: 2.6603\n",
      "Epoch [41/100], Train Loss: 2.4139, Validation Loss: 2.6969\n",
      "Epoch [51/100], Train Loss: 2.3885, Validation Loss: 2.7158\n",
      "Epoch [61/100], Train Loss: 2.3672, Validation Loss: 2.7346\n",
      "Epoch [71/100], Train Loss: 2.3641, Validation Loss: 2.7352\n",
      "Epoch [81/100], Train Loss: 2.3472, Validation Loss: 2.7538\n",
      "Epoch [91/100], Train Loss: 2.3478, Validation Loss: 2.7884\n",
      "Test Loss: 2.7773, Test Accuracy: 0.3355, Test F1: 0.0473\n",
      "Running experiment for graph: mag with feature: ga\n",
      "Epoch [1/100], Train Loss: 3.0255, Validation Loss: 2.6476\n",
      "Epoch [11/100], Train Loss: 2.5585, Validation Loss: 2.6049\n",
      "Epoch [21/100], Train Loss: 2.4831, Validation Loss: 2.6171\n",
      "Epoch [31/100], Train Loss: 2.4439, Validation Loss: 2.6350\n",
      "Epoch [41/100], Train Loss: 2.4160, Validation Loss: 2.6524\n",
      "Epoch [51/100], Train Loss: 2.3944, Validation Loss: 2.6780\n",
      "Epoch [61/100], Train Loss: 2.3904, Validation Loss: 2.7021\n",
      "Epoch [71/100], Train Loss: 2.3852, Validation Loss: 2.7287\n",
      "Epoch [81/100], Train Loss: 2.3726, Validation Loss: 2.7159\n",
      "Epoch [91/100], Train Loss: 2.3674, Validation Loss: 2.7430\n",
      "Test Loss: 3.0654, Test Accuracy: 0.2997, Test F1: 0.0393\n",
      "Running experiment for graph: arxiv with feature: orig\n",
      "Epoch [1/100], Train Loss: 2.3474, Validation Loss: 1.7340\n",
      "Epoch [11/100], Train Loss: 1.4870, Validation Loss: 1.4827\n",
      "Epoch [21/100], Train Loss: 1.3745, Validation Loss: 1.4519\n",
      "Epoch [31/100], Train Loss: 1.3015, Validation Loss: 1.4626\n",
      "Epoch [41/100], Train Loss: 1.2417, Validation Loss: 1.4570\n",
      "Epoch [51/100], Train Loss: 1.2011, Validation Loss: 1.4776\n",
      "Epoch [61/100], Train Loss: 1.1660, Validation Loss: 1.4951\n",
      "Epoch [71/100], Train Loss: 1.1335, Validation Loss: 1.5186\n",
      "Epoch [81/100], Train Loss: 1.1019, Validation Loss: 1.5287\n",
      "Epoch [91/100], Train Loss: 1.0866, Validation Loss: 1.5348\n",
      "Test Loss: 1.5944, Test Accuracy: 0.5462, Test F1: 0.3339\n",
      "Running experiment for graph: arxiv with feature: e5\n",
      "Epoch [1/100], Train Loss: 2.2495, Validation Loss: 1.7151\n",
      "Epoch [11/100], Train Loss: 1.7500, Validation Loss: 1.7082\n",
      "Epoch [21/100], Train Loss: 1.6461, Validation Loss: 1.7620\n",
      "Epoch [31/100], Train Loss: 1.5944, Validation Loss: 1.8165\n",
      "Epoch [41/100], Train Loss: 1.5612, Validation Loss: 1.8571\n",
      "Epoch [51/100], Train Loss: 1.5353, Validation Loss: 1.8738\n",
      "Epoch [61/100], Train Loss: 1.5178, Validation Loss: 1.9075\n",
      "Epoch [71/100], Train Loss: 1.5057, Validation Loss: 1.9310\n",
      "Epoch [81/100], Train Loss: 1.4854, Validation Loss: 1.9455\n",
      "Epoch [91/100], Train Loss: 1.4778, Validation Loss: 1.9671\n",
      "Test Loss: 2.9352, Test Accuracy: 0.1242, Test F1: 0.0931\n",
      "Running experiment for graph: arxiv with feature: ga\n",
      "Epoch [1/100], Train Loss: 2.8786, Validation Loss: 2.8687\n",
      "Epoch [11/100], Train Loss: 2.3168, Validation Loss: 2.7918\n",
      "Epoch [21/100], Train Loss: 2.2602, Validation Loss: 2.7757\n",
      "Epoch [31/100], Train Loss: 2.2193, Validation Loss: 2.7918\n",
      "Epoch [41/100], Train Loss: 2.1968, Validation Loss: 2.7966\n",
      "Epoch [51/100], Train Loss: 2.1731, Validation Loss: 2.8008\n",
      "Epoch [61/100], Train Loss: 2.1610, Validation Loss: 2.7871\n",
      "Epoch [71/100], Train Loss: 2.1410, Validation Loss: 2.7932\n",
      "Epoch [81/100], Train Loss: 2.1326, Validation Loss: 2.8005\n",
      "Epoch [91/100], Train Loss: 2.1166, Validation Loss: 2.7800\n",
      "Test Loss: 3.0286, Test Accuracy: 0.0862, Test F1: 0.0232\n"
     ]
    }
   ],
   "source": [
    "graphs = {\"joinned\": joinned_graph, \"mag\":mag_graph, \"arxiv\":arxiv_graph}\n",
    "features = ['orig', 'e5', 'ga']\n",
    "\n",
    "combinations = list(itertools.product(graphs.keys(), features))\n",
    "\n",
    "results_dict = {}\n",
    "for pairs in combinations:\n",
    "    print(f\"Running experiment for graph: {pairs[0]} with feature: {pairs[1]}\")\n",
    "    results_dict[pairs] = experiment(graphs[pairs[0]], pairs[1], in_feats, hidden_size, num_classes, dropout_rate, batch_size, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf09b220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Graph</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Best Val Epoch</th>\n",
       "      <th>Best_Val_Accuracy</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Joinned</td>\n",
       "      <td>Original</td>\n",
       "      <td>18</td>\n",
       "      <td>0.4276</td>\n",
       "      <td>0.4774</td>\n",
       "      <td>0.4501</td>\n",
       "      <td>0.0775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Joinned</td>\n",
       "      <td>E5 Embedding</td>\n",
       "      <td>10</td>\n",
       "      <td>0.3814</td>\n",
       "      <td>0.4285</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.0472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Joinned</td>\n",
       "      <td>Graph Algin Embedding</td>\n",
       "      <td>28</td>\n",
       "      <td>0.2747</td>\n",
       "      <td>0.3899</td>\n",
       "      <td>0.1565</td>\n",
       "      <td>0.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>MAG</td>\n",
       "      <td>Original</td>\n",
       "      <td>56</td>\n",
       "      <td>0.3427</td>\n",
       "      <td>0.4281</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>0.0502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>MAG</td>\n",
       "      <td>E5 Embedding</td>\n",
       "      <td>24</td>\n",
       "      <td>0.3327</td>\n",
       "      <td>0.4470</td>\n",
       "      <td>0.3355</td>\n",
       "      <td>0.0473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>MAG</td>\n",
       "      <td>Graph Algin Embedding</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3457</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0.2997</td>\n",
       "      <td>0.0393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Arxiv</td>\n",
       "      <td>Original</td>\n",
       "      <td>21</td>\n",
       "      <td>0.5755</td>\n",
       "      <td>0.7742</td>\n",
       "      <td>0.5462</td>\n",
       "      <td>0.3339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Arxiv</td>\n",
       "      <td>E5 Embedding</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5072</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>0.0931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP</td>\n",
       "      <td>Arxiv</td>\n",
       "      <td>Graph Algin Embedding</td>\n",
       "      <td>77</td>\n",
       "      <td>0.2016</td>\n",
       "      <td>0.4633</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model    Graph                Feature  Best Val Epoch  Best_Val_Accuracy  \\\n",
       "0   MLP  Joinned               Original              18             0.4276   \n",
       "1   MLP  Joinned           E5 Embedding              10             0.3814   \n",
       "2   MLP  Joinned  Graph Algin Embedding              28             0.2747   \n",
       "3   MLP      MAG               Original              56             0.3427   \n",
       "4   MLP      MAG           E5 Embedding              24             0.3327   \n",
       "5   MLP      MAG  Graph Algin Embedding              11             0.3457   \n",
       "6   MLP    Arxiv               Original              21             0.5755   \n",
       "7   MLP    Arxiv           E5 Embedding               6             0.5072   \n",
       "8   MLP    Arxiv  Graph Algin Embedding              77             0.2016   \n",
       "\n",
       "   Train_Accuracy  Test_Accuracy  Test_f1  \n",
       "0          0.4774         0.4501   0.0775  \n",
       "1          0.4285         0.1935   0.0472  \n",
       "2          0.3899         0.1565   0.0310  \n",
       "3          0.4281         0.3372   0.0502  \n",
       "4          0.4470         0.3355   0.0473  \n",
       "5          0.4027         0.2997   0.0393  \n",
       "6          0.7742         0.5462   0.3339  \n",
       "7          0.6857         0.1242   0.0931  \n",
       "8          0.4633         0.0862   0.0232  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict, orient='index')\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.columns = ['Graph', 'Feature', 'Best Val Epoch', 'Training Loss (Best Val Model)', 'Best_Val_Accuracy', 'Train_Accuracy', 'Test_Accuracy', 'Test_f1']\n",
    "results_df['Model'] = 'MLP'\n",
    "results_df['Graph'] = results_df['Graph'].map(lambda x: {'joinned': 'Joinned', 'mag': 'MAG', 'arxiv': 'Arxiv'}[x])\n",
    "results_df['Feature'] = results_df['Feature'].map(lambda x: {'orig': 'Original', 'e5': 'E5 Embedding', 'ga': 'Graph Algin Embedding'}[x])\n",
    "results_df[['Model','Graph', 'Feature', 'Best Val Epoch', 'Best_Val_Accuracy', 'Train_Accuracy', 'Test_Accuracy', 'Test_f1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80ba43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('Experiment_Results/MLP.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
